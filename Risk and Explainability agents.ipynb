{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96970d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\anaconda3\\envs\\tf310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\91939\\anaconda3\\envs\\tf310\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import shap\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.tools import FunctionTool\n",
    "import uuid\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d6d8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model Training & Calibration ==============================================\n",
    "def train_and_save_model(X_train, y_train, numeric_features, categorical_features):\n",
    "    # Preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Base pipeline\n",
    "    base_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', xgb.XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='logloss',\n",
    "            scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum()\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train and calibrate\n",
    "    base_pipeline.fit(X_train, y_train)\n",
    "    calibrated_pipeline = CalibratedClassifierCV(base_pipeline, method='isotonic', cv='prefit')\n",
    "    calibrated_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(calibrated_pipeline, 'insurance_risk_model.joblib')\n",
    "    print(\"Model trained and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca401924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsuranceRiskAgents:\n",
    "    def __init__(self):\n",
    "        # Load model and background data\n",
    "        model_data = joblib.load(\"calibrated_risk_model.joblib\")\n",
    "        self.model = model_data['calibrated_model']\n",
    "        self.background_data = model_data['background_data']\n",
    "        \n",
    "        # Access base pipeline through correct attribute\n",
    "        self.base_pipeline = self.model.calibrated_classifiers_[0].estimator\n",
    "        self.preprocessor = self.base_pipeline.named_steps['preprocessor']\n",
    "        self.classifier = self.base_pipeline.named_steps['classifier']\n",
    "        \n",
    "        # Initialize SHAP explainer with proper data\n",
    "        self.feature_names = self._get_feature_names()\n",
    "        self.explainer = self._create_shap_explainer()\n",
    "        self.model_version = \"1.0.20240615\"\n",
    "\n",
    "    def _get_feature_names(self):\n",
    "        \"\"\"Get feature names from preprocessor\"\"\"\n",
    "        numeric_features = self.preprocessor.transformers_[0][2]\n",
    "        categorical_features = list(\n",
    "            self.preprocessor.named_transformers_['cat']\n",
    "            .get_feature_names_out(self.preprocessor.transformers_[1][2])\n",
    "        )\n",
    "        return numeric_features + categorical_features\n",
    "\n",
    "    def _create_shap_explainer(self):\n",
    "        \"\"\"Initialize SHAP explainer with correct data types\"\"\"\n",
    "        return shap.TreeExplainer(\n",
    "            self.classifier,\n",
    "            data=self.background_data,\n",
    "            feature_perturbation=\"interventional\",\n",
    "            model_output=\"probability\"\n",
    "        )\n",
    "\n",
    "    def predict_risk(self, input_data: dict) -> dict:\n",
    "        \"\"\"Generate risk prediction with calibrated probabilities\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame([input_data])\n",
    "            processed = self.preprocessor.transform(df)\n",
    "            proba = self.model.predict_proba(processed)[0, 1]\n",
    "            return {\n",
    "                'risk_score': float(proba),\n",
    "                'confidence': float(abs(proba - 0.5) * 2),\n",
    "                'model_version': self.model_version\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'error': f\"Prediction failed: {str(e)}\",\n",
    "                'model_version': self.model_version\n",
    "            }\n",
    "\n",
    "    def explain_risk(self, input_data: dict, tool_context=None) -> dict:\n",
    "        \"\"\"Generate regulator-ready explanation with audit trail\"\"\"\n",
    "        try:\n",
    "            # Handle session state if context exists\n",
    "            if tool_context:\n",
    "                history = tool_context.state.get(\"conversation_history\", [])\n",
    "                history.append({\n",
    "                    \"input\": input_data,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                })\n",
    "                tool_context.state[\"conversation_history\"] = history\n",
    "\n",
    "            # Generate SHAP explanation\n",
    "            df = pd.DataFrame([input_data])\n",
    "            processed = self.preprocessor.transform(df)\n",
    "            shap_values = self.explainer.shap_values(processed)[0]\n",
    "            \n",
    "            # Format output\n",
    "            top_factors = self._format_shap_factors(shap_values)\n",
    "            plot_path = self._generate_shap_plot(shap_values)\n",
    "            audit_id = str(uuid.uuid4())\n",
    "\n",
    "            # Add audit log if context exists\n",
    "            if tool_context:\n",
    "                audit_log = tool_context.state.get(\"audit_log\", [])\n",
    "                audit_log.append({\n",
    "                    \"audit_id\": audit_id,\n",
    "                    \"input\": input_data,\n",
    "                    \"result\": top_factors,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                })\n",
    "                tool_context.state[\"audit_log\"] = audit_log\n",
    "\n",
    "            return {\n",
    "                'factors': top_factors,\n",
    "                'plot_path': plot_path,\n",
    "                'compliance_note': \"Explanation compliant with EU AI Act Article 13\",\n",
    "                'model_version': self.model_version,\n",
    "                'audit_id': audit_id\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'error': f\"Explanation failed: {str(e)}\",\n",
    "                'model_version': self.model_version\n",
    "            }\n",
    "\n",
    "    def _format_shap_factors(self, shap_values):\n",
    "        \"\"\"Format top 3 risk factors with impact scores\"\"\"\n",
    "        top_idx = np.argsort(-np.abs(shap_values))[:3]\n",
    "        return [\n",
    "            f\"{self.feature_names[i]} ({shap_values[i]:+.2f} impact)\"\n",
    "            for i in top_idx\n",
    "        ]\n",
    "\n",
    "    def _generate_shap_plot(self, shap_values):\n",
    "        \"\"\"Generate and save SHAP decision plot\"\"\"\n",
    "        plot_id = str(uuid.uuid4())\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        shap.decision_plot(\n",
    "            self.explainer.expected_value,\n",
    "            shap_values,\n",
    "            self.feature_names,\n",
    "            show=False\n",
    "        )\n",
    "        plt.title(\"Risk Decision Breakdown\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plot_path = f\"shap_plot_{plot_id}.png\"\n",
    "        plt.savefig(plot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        return plot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5aae89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsuranceRiskAgents:\n",
    "    def __init__(self):\n",
    "        # Load model and background data\n",
    "        model_data = joblib.load(\"calibrated_risk_model.joblib\")\n",
    "        self.model = model_data['calibrated_model']\n",
    "        self.background_data = model_data['background_data']\n",
    "        \n",
    "        # Access base pipeline through correct attribute\n",
    "        self.base_pipeline = self.model.calibrated_classifiers_[0].estimator\n",
    "        self.preprocessor = self.base_pipeline.named_steps['preprocessor']\n",
    "        self.classifier = self.base_pipeline.named_steps['classifier']\n",
    "        \n",
    "        # Track original feature names (critical fix)\n",
    "        self.original_feature_names = (\n",
    "            self.preprocessor.transformers_[0][2] +  # numeric\n",
    "            self.preprocessor.transformers_[1][2]    # categorical\n",
    "        )\n",
    "        \n",
    "        # Initialize SHAP explainer\n",
    "        self.feature_names = self._get_feature_names()\n",
    "        self.explainer = self._create_shap_explainer()\n",
    "        self.model_version = \"1.0.20240615\"\n",
    "\n",
    "    def _get_feature_names(self):\n",
    "        \"\"\"Get transformed feature names from preprocessor\"\"\"\n",
    "        numeric = self.preprocessor.transformers_[0][2]\n",
    "        categorical = list(\n",
    "            self.preprocessor.named_transformers_['cat']\n",
    "            .get_feature_names_out(self.preprocessor.transformers_[1][2])\n",
    "        )\n",
    "        return numeric + categorical\n",
    "\n",
    "    def _create_shap_explainer(self):\n",
    "        \"\"\"Initialize SHAP explainer with correct data types\"\"\"\n",
    "        return shap.TreeExplainer(\n",
    "            self.classifier,\n",
    "            data=self.background_data,\n",
    "            feature_perturbation=\"interventional\",\n",
    "            model_output=\"probability\"\n",
    "        )\n",
    "\n",
    "    def _validate_input(self, input_data: dict) -> bool:\n",
    "        \"\"\"Ensure input contains all required raw features\"\"\"\n",
    "        return all(feat in input_data for feat in self.original_feature_names)\n",
    "\n",
    "    def predict_risk(self, input_data: dict) -> dict:\n",
    "        \"\"\"Generate risk prediction with calibrated probabilities\"\"\"\n",
    "        if not self._validate_input(input_data):\n",
    "            return {\n",
    "                'error': f\"Missing features. Required: {self.original_feature_names}\",\n",
    "                'model_version': self.model_version\n",
    "            }\n",
    "            \n",
    "        try:\n",
    "            df = pd.DataFrame([input_data])\n",
    "            proba = self.model.predict_proba(df)[0, 1]\n",
    "            return {\n",
    "                'risk_score': float(proba),\n",
    "                'confidence': float(abs(proba - 0.5) * 2),\n",
    "                'model_version': self.model_version\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'error': f\"Prediction failed: {str(e)}\",\n",
    "                'model_version': self.model_version\n",
    "            }\n",
    "\n",
    "    def explain_risk(self, input_data: dict, tool_context=None) -> dict:\n",
    "        \"\"\"Generate regulator-ready explanation with audit trail\"\"\"\n",
    "        if not self._validate_input(input_data):\n",
    "            return {\n",
    "                'error': f\"Missing features. Required: {self.original_feature_names}\",\n",
    "                'model_version': self.model_version\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            # Handle session state if context exists\n",
    "            if tool_context:\n",
    "                history = tool_context.state.get(\"conversation_history\", [])\n",
    "                history.append({\n",
    "                    \"input\": input_data,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                })\n",
    "                tool_context.state[\"conversation_history\"] = history\n",
    "\n",
    "            # Generate SHAP explanation\n",
    "            df = pd.DataFrame([input_data])\n",
    "            processed = self.preprocessor.transform(df)\n",
    "            shap_values = self.explainer.shap_values(processed)[0]\n",
    "            \n",
    "            # Format output\n",
    "            top_factors = self._format_shap_factors(shap_values)\n",
    "            plot_path = self._generate_shap_plot(shap_values)\n",
    "            audit_id = str(uuid.uuid4())\n",
    "\n",
    "            # Add audit log if context exists\n",
    "            if tool_context:\n",
    "                audit_log = tool_context.state.get(\"audit_log\", [])\n",
    "                audit_log.append({\n",
    "                    \"audit_id\": audit_id,\n",
    "                    \"input\": input_data,\n",
    "                    \"result\": top_factors,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                })\n",
    "                tool_context.state[\"audit_log\"] = audit_log\n",
    "\n",
    "            return {\n",
    "                'factors': top_factors,\n",
    "                'plot_path': plot_path,\n",
    "                'compliance_note': \"Explanation compliant with EU AI Act Article 13\",\n",
    "                'model_version': self.model_version,\n",
    "                'audit_id': audit_id\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'error': f\"Explanation failed: {str(e)}\",\n",
    "                'model_version': self.model_version\n",
    "            }\n",
    "\n",
    "    def _format_shap_factors(self, shap_values):\n",
    "        \"\"\"Format top 3 risk factors with impact scores\"\"\"\n",
    "        top_idx = np.argsort(-np.abs(shap_values))[:3]\n",
    "        return [\n",
    "            f\"{self.feature_names[i]} ({shap_values[i]:+.2f} impact)\"\n",
    "            for i in top_idx\n",
    "        ]\n",
    "\n",
    "    def _generate_shap_plot(self, shap_values):\n",
    "        \"\"\"Generate and save SHAP decision plot\"\"\"\n",
    "        plot_id = str(uuid.uuid4())\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        shap.decision_plot(\n",
    "            self.explainer.expected_value,\n",
    "            shap_values,\n",
    "            self.feature_names,\n",
    "            show=False\n",
    "        )\n",
    "        plt.title(\"Risk Decision Breakdown\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plot_path = f\"shap_plot_{plot_id}.png\"\n",
    "        plt.savefig(plot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        return plot_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263911e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize service components\n",
    "# Create agents with PROPER parameters\n",
    "risk_service = InsuranceRiskAgents()\n",
    "predict_risk_tool = FunctionTool(risk_service.predict_risk)\n",
    "explain_risk_tool = FunctionTool(risk_service.explain_risk)\n",
    "risk_agent = Agent(\n",
    "    name=\"risk_modeling_agent\",\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    tools=[predict_risk_tool],\n",
    "    instruction=\"Provide risk scores with confidence intervals\",\n",
    "    description=\"Production risk scoring engine\"\n",
    ")\n",
    "\n",
    "explain_agent = Agent(\n",
    "    name=\"compliance_explainer\",\n",
    "    model=\"gemini-2.0-pro\",\n",
    "    tools=[explain_risk_tool],\n",
    "    instruction=\"Generate regulator-friendly explanations with SHAP plots\",\n",
    "    description=\"Enterprise explainability service\"\n",
    ")\n",
    "\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "\n",
    "# Allow nested async in Jupyter/Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize session service\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "# Create session with proper async handling\n",
    "session = asyncio.get_event_loop().run_until_complete(\n",
    "    session_service.create_session(\n",
    "        app_name=\"insurance_risk\",\n",
    "        user_id=\"regulator_123\",\n",
    "        session_id=\"audit_456\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Now access state safely\n",
    "session.state[\"model_version\"] = risk_service.model_version\n",
    "session.state[\"conversation_history\"] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6b5acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk Prediction: {'risk_score': 0.9766454100608826, 'confidence': 0.9532908201217651, 'model_version': '1.0.20240615'}\n",
      "Explanation: {'factors': ['vehicle_age (+0.18 impact)', 'policy_type_Comprehensive (+0.14 impact)', 'age (+0.05 impact)'], 'plot_path': 'shap_plot_de4cb2f0-39d5-4459-882c-bc7a8fd42ebb.png', 'compliance_note': 'Explanation compliant with EU AI Act Article 13', 'model_version': '1.0.20240615', 'audit_id': 'b865ebcd-f222-4ded-ac63-0aa62f054a1d'}\n"
     ]
    }
   ],
   "source": [
    "# Usage Example ===============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # First train and save model (do once)\n",
    "    # train_and_save_model(X_train, y_train, numeric_features, categorical_features)\n",
    "    \n",
    "    # Initialize agents\n",
    "    agents = InsuranceRiskAgents()\n",
    "    \n",
    "    # Sample prediction\n",
    "    sample_input = {\n",
    "    'age': 45,\n",
    "    'income': 85000,\n",
    "    'vehicle_age': 3,                # Added missing numeric feature\n",
    "    'vehicle_value': 35000,\n",
    "    'premium_amount': 1200,          # Added missing numeric feature\n",
    "    'vehicle_brand': 'Toyota',       # Added categorical features\n",
    "    'occupation': 'engineer',\n",
    "    'city': 'Metropolis',\n",
    "    'policy_type': 'Comprehensive',\n",
    "    'gender': 'M'}\n",
    "    \n",
    "    prediction = agents.predict_risk(sample_input)\n",
    "    explanation = agents.explain_risk(sample_input)\n",
    "    \n",
    "    print(\"Risk Prediction:\", prediction)\n",
    "    print(\"Explanation:\", explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ddeb54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did I get a high risk score?\n",
      "Answer: ['vehicle_age (+0.18 impact)', 'policy_type_Comprehensive (+0.14 impact)', 'age (+0.05 impact)']\n"
     ]
    }
   ],
   "source": [
    "# After running a prediction\n",
    "explanation = agents.explain_risk(sample_input)\n",
    "print(\"Why did I get a high risk score?\")\n",
    "print(\"Answer:\", explanation['factors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcabee4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is this a good score?\n",
      "Answer: ['vehicle_age (+0.18 impact)', 'policy_type_Comprehensive (+0.14 impact)', 'age (+0.05 impact)']\n"
     ]
    }
   ],
   "source": [
    "# After running a prediction\n",
    "explanation = agents.explain_risk(sample_input)\n",
    "print(\"Is this a good score?\")\n",
    "print(\"Answer:\", explanation['factors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a332dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
